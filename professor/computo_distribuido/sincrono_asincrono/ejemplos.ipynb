{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Síncrono vs Asíncrono: Notebook de ejemplos prácticos\n",
        "\n",
        "Este notebook acompaña al documento de lectura y lleva las ideas a la práctica con código mínimo y comentarios detallados. Buscamos entender cómo se comporta el tiempo total y qué ocurre “entre bastidores”.\n",
        "\n",
        "Cómo usar este notebook:\n",
        "- Ejecuta las celdas en orden. Cada sección explica primero el concepto y luego muestra código.\n",
        "- Observa los tiempos impresos; compáralos entre enfoques (síncrono, hilos, asíncrono, procesos).\n",
        "- Prueba cambiando parámetros (p. ej., número de tareas, `max_workers`, duraciones de `sleep`) y vuelve a ejecutar para ver cómo cambia el resultado.\n",
        "\n",
        "Guía de navegación:\n",
        "- Utilidades para medir tiempo y observar memoria\n",
        "- Sencillo y bloqueante (síncrono)\n",
        "- Concurrencia con hilos (I/O simulado)\n",
        "- Asíncrono con asyncio (I/O no bloqueante)\n",
        "- Paralelismo con procesos (CPU-bound)\n",
        "- Casos edge y combinaciones\n",
        "- Notas rápidas sobre memoria y GIL\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utilidades: medir tiempo y observar memoria\n",
        "\n",
        "Antes de comparar enfoques, necesitamos medir tiempo y (de forma muy básica) observar memoria:\n",
        "- `time.perf_counter()`: cronómetro de alta resolución ideal para medir duración de bloques.\n",
        "- Memoria de proceso: usaremos `resource` (Linux/macOS) o, en Linux, `/proc/self/status` para leer `VmRSS` y `VmSize`.\n",
        "- Helpers que usaremos a lo largo del notebook:\n",
        "  - `time_block(label)`: mide y muestra el tiempo de ejecución de un bloque de código.\n",
        "  - `get_memory_info()`: imprime una vista rápida del uso de memoria del proceso (aproximada; suficiente para fines didácticos).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mem info inicial: ru_maxrss: 134472 KB (interprete del SO) | VmSize:\t 1058292 kB | VmRSS:\t   68328 kB\n",
            "[pausa de 0.1s] tiempo: 0.101 s\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "from contextlib import contextmanager\n",
        "\n",
        "# Intentamos importar 'resource' para Linux/macOS\n",
        "try:\n",
        "    import resource  # type: ignore\n",
        "    _HAS_RESOURCE = True\n",
        "except Exception:\n",
        "    _HAS_RESOURCE = False\n",
        "\n",
        "@contextmanager\n",
        "def time_block(label: str = \"tarea\"):\n",
        "    \"\"\"Mide el tiempo transcurrido dentro del bloque.\n",
        "    Uso:\n",
        "    with time_block(\"descargas\"):\n",
        "        ...\n",
        "    \"\"\"\n",
        "    t0 = time.perf_counter()\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        dt = time.perf_counter() - t0\n",
        "        print(f\"[{label}] tiempo: {dt:.3f} s\")\n",
        "\n",
        "\n",
        "def get_memory_info():\n",
        "    \"\"\"Devuelve info de memoria del proceso actual en una tupla (rss_kb, texto_libre).\n",
        "    - En Linux/macOS, intenta usar 'resource' (ru_maxrss)\n",
        "    - En Linux, como alternativa, lee /proc/self/status\n",
        "    \"\"\"\n",
        "    rss_kb = None\n",
        "    text = []\n",
        "\n",
        "    if _HAS_RESOURCE:\n",
        "        try:\n",
        "            usage = resource.getrusage(resource.RUSAGE_SELF)\n",
        "            # Nota: en Linux ru_maxrss suele ser KB; en macOS, bytes/1024\n",
        "            rss_kb = int(usage.ru_maxrss)\n",
        "            text.append(f\"ru_maxrss: {rss_kb} KB (interprete del SO)\")\n",
        "        except Exception as e:\n",
        "            text.append(f\"resource fallo: {e}\")\n",
        "\n",
        "    # Intento /proc/self/status (Linux)\n",
        "    status_path = \"/proc/self/status\"\n",
        "    if os.path.exists(status_path):\n",
        "        try:\n",
        "            with open(status_path, \"r\") as f:\n",
        "                for line in f:\n",
        "                    if line.startswith(\"VmRSS:\"):\n",
        "                        text.append(line.strip())\n",
        "                    if line.startswith(\"VmSize:\"):\n",
        "                        text.append(line.strip())\n",
        "        except Exception as e:\n",
        "            text.append(f\"/proc/self/status fallo: {e}\")\n",
        "\n",
        "    return rss_kb, \" | \".join(text) if text else \"sin datos\"\n",
        "\n",
        "\n",
        "# Prueba rápida de utilidades\n",
        "rss_kb, info = get_memory_info()\n",
        "print(\"Mem info inicial:\", info)\n",
        "with time_block(\"pausa de 0.1s\"):\n",
        "    time.sleep(0.1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Sencillo y bloqueante (síncrono)\n",
        "En el estilo síncrono, cada paso espera a que termine el anterior. La ventaja es la simplicidad; la desventaja es que el tiempo total crece con cada espera.\n",
        "\n",
        "Qué observar:\n",
        "- Con 5 tareas de 0.5 s cada una, el tiempo total será cercano a ~2.5 s.\n",
        "- Cambia el número de tareas o la duración del `sleep` y compara con las siguientes secciones.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[sincrono: 5 tareas * 0.5s] tiempo: 2.510 s\n",
            "Resultados: ['ok 0', 'ok 1', 'ok 2', 'ok 3', 'ok 4']\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "def tarea_bloqueante(i: int) -> str:\n",
        "    # Simula una operación que tarda (p. ej., E/S)\n",
        "    time.sleep(0.5)\n",
        "    return f\"ok {i}\"\n",
        "\n",
        "with time_block(\"sincrono: 5 tareas * 0.5s\"):\n",
        "    resultados = []\n",
        "    for i in range(5):\n",
        "        # No se inicia la siguiente hasta que termina la actual\n",
        "        resultados.append(tarea_bloqueante(i))\n",
        "\n",
        "print(\"Resultados:\", resultados)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Concurrencia con hilos (I/O simulado)\n",
        "Creamos un grupo de hilos (“trabajadores”). Cuando un hilo queda esperando E/S, otro hilo puede ejecutar otra tarea.\n",
        "\n",
        "Qué observar:\n",
        "- Con 5 workers y 5 tareas de 0.5 s, el tiempo total se acerca a ~0.5–0.6 s (más sobrecarga), porque varias tareas progresan durante el mismo periodo.\n",
        "- `as_completed` permite procesar resultados en cuanto estén listos.\n",
        "Nota: para trabajo de CPU (no E/S), los hilos en CPython no dan paralelismo real por el GIL; veremos procesos más adelante.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[hilos: 5 tareas * 0.5s con 5 workers] tiempo: 0.506 s\n",
            "Resultados (orden de finalización): ['ok 0', 'ok 2', 'ok 1', 'ok 3', 'ok 4']\n"
          ]
        }
      ],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "import time\n",
        "\n",
        "def tarea_io(i: int) -> str:\n",
        "    # Simula una espera I/O\n",
        "    time.sleep(0.5)\n",
        "    return f\"ok {i}\"\n",
        "\n",
        "with time_block(\"hilos: 5 tareas * 0.5s con 5 workers\"):\n",
        "    with ThreadPoolExecutor(max_workers=5) as ex:\n",
        "        futuros = [ex.submit(tarea_io, i) for i in range(5)]\n",
        "        # Procesamos en el orden en que terminen (opcional)\n",
        "        resultados = [f.result() for f in as_completed(futuros)]\n",
        "\n",
        "print(\"Resultados (orden de finalización):\", resultados)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1) Hilos y estado compartido: carrera y Lock\n",
        "Si varios hilos escriben el mismo dato sin coordinación, aparecen condiciones de carrera: el resultado depende del intercalado de operaciones y puede variar entre ejecuciones.\n",
        "\n",
        "Qué observar:\n",
        "- Caso sin lock: el contador final suele ser menor que el esperado (se pierden incrementos).\n",
        "- Caso con lock: protegemos la sección crítica y el resultado coincide con lo esperado.\n",
        "Consejo: mantén al mínimo las secciones críticas (dentro del `with lock:`) para reducir contención y evitar bloqueos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[hilos sin lock] tiempo: 0.046 s\n",
            "Esperado: 200000 Obtenido: 200000\n",
            "[hilos con lock] tiempo: 0.072 s\n",
            "Esperado: 200000 Obtenido: 200000\n"
          ]
        }
      ],
      "source": [
        "import threading\n",
        "\n",
        "N = 100_000\n",
        "contador = 0\n",
        "\n",
        "# Caso sin lock (puede perder incrementos)\n",
        "def inc_sin_lock():\n",
        "    global contador\n",
        "    for _ in range(N):\n",
        "        contador += 1  # no atómico\n",
        "\n",
        "# Caso con lock (correcto)\n",
        "contador2 = 0\n",
        "lock = threading.Lock()\n",
        "\n",
        "def inc_con_lock():\n",
        "    global contador2\n",
        "    for _ in range(N):\n",
        "        with lock:  # sección crítica protegida\n",
        "            contador2 += 1\n",
        "\n",
        "# Ejecutamos ambos casos\n",
        "h1 = threading.Thread(target=inc_sin_lock)\n",
        "h2 = threading.Thread(target=inc_sin_lock)\n",
        "with time_block(\"hilos sin lock\"):\n",
        "    h1.start(); h2.start(); h1.join(); h2.join()\n",
        "print(\"Esperado:\", 2*N, \"Obtenido:\", contador)\n",
        "\n",
        "h3 = threading.Thread(target=inc_con_lock)\n",
        "h4 = threading.Thread(target=inc_con_lock)\n",
        "with time_block(\"hilos con lock\"):\n",
        "    h3.start(); h4.start(); h3.join(); h4.join()\n",
        "print(\"Esperado:\", 2*N, \"Obtenido:\", contador2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Asíncrono con asyncio (I/O no bloqueante)\n",
        "\n",
        "Cuando una tarea necesita esperar (red, disco, temporizadores), puede “ceder el turno” para que otras tareas avancen en el mismo hilo. `asyncio` implementa este estilo cooperativo:\n",
        "\n",
        "- Event loop: un coordinador que reanuda tareas cuando termina su espera.\n",
        "- `async`/`await`: puntos donde la tarea se pausa y el event loop puede continuar con otra.\n",
        "- Ideal para I/O-bound (muchas esperas); no proporciona paralelismo de CPU por sí solo.\n",
        "\n",
        "Idea visual (línea de tiempo):\n",
        "```\n",
        "A: inicia ── await(I/O) ──▶ retoma\n",
        "B:          avanza mientras A espera ─────────▶\n",
        "C:                    avanza mientras A espera ──▶\n",
        "```\n",
        "\n",
        "Buenas prácticas:\n",
        "- Evita llamadas bloqueantes dentro de funciones `async` (añaden demoras a todo el conjunto).\n",
        "- En notebooks, usa top-level `await` (no `asyncio.run`) porque ya hay un event loop activo.\n",
        "- Para CPU-bound, combina con procesos; para I/O sin bloquear, usa `await` en operaciones no bloqueantes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resultados: ['ok 0', 'ok 1', 'ok 2', 'ok 3', 'ok 4']\n",
            "[asyncio: 5 tareas * 0.5s] tiempo: 0.503 s\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "\n",
        "async def tarea_io_async(i: int) -> str:\n",
        "    # Simula E/S no bloqueante: cede el control por 0.5s\n",
        "    await asyncio.sleep(0.5)\n",
        "    return f\"ok {i}\"\n",
        "\n",
        "async def run_async():\n",
        "    # Lanzamos varias tareas y esperamos a que todas terminen\n",
        "    tareas = [asyncio.create_task(tarea_io_async(i)) for i in range(5)]\n",
        "    return await asyncio.gather(*tareas)\n",
        "\n",
        "async def _driver():\n",
        "    # En notebooks, el event loop ya está activo; usamos await en vez de asyncio.run\n",
        "    with time_block(\"asyncio: 5 tareas * 0.5s\"):\n",
        "        resultados = await run_async()\n",
        "        print(\"Resultados:\", resultados)\n",
        "\n",
        "# Top-level await funciona en Jupyter/IPython\n",
        "await _driver()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1) Asíncrono sin concurrencia efectiva\n",
        "Aquí usamos `await` de forma secuencial: esperamos a que termine la tarea 1 para iniciar la 2, y así sucesivamente. Resultado: el tiempo total crece con el número de tareas.\n",
        "\n",
        "Cómo hacerlo concurrente: crea todas las tareas primero y espera a todas con `gather` (ver celda anterior de asyncio)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4]\n",
            "[asyncio secuencial: 5 * 0.3s] tiempo: 1.506 s\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "\n",
        "async def run_async_secuencial():\n",
        "    r = []\n",
        "    # Ejecutamos una tras otra; el tiempo total será cercano a 5 * 0.3s\n",
        "    for i in range(5):\n",
        "        await asyncio.sleep(0.3)\n",
        "        r.append(i)\n",
        "    return r\n",
        "\n",
        "async def _driver_seq():\n",
        "    # En notebooks, evitamos asyncio.run y usamos top-level await\n",
        "    with time_block(\"asyncio secuencial: 5 * 0.3s\"):\n",
        "        print(await run_async_secuencial())\n",
        "\n",
        "await _driver_seq()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Paralelismo con procesos (CPU-bound)\n",
        "Para trabajo intensivo de CPU, varios procesos pueden ejecutar realmente al mismo tiempo en distintos núcleos. A diferencia de los hilos en CPython, cada proceso tiene su propio intérprete y no comparten el mismo GIL.\n",
        "\n",
        "Qué observar:\n",
        "- Comparación threads vs processes: en CPU-bound, procesos suelen ser más rápidos (paralelismo real).\n",
        "- Hay sobrecarga de crear procesos y de enviar/recibir datos (serialización).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[threads CPU-bound (GIL limita)] tiempo: 0.129 s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/connection.py:766: RuntimeWarning: coroutine 'run_async' was never awaited\n",
            "  def __init__(self, conn, dumps, loads):\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[processes CPU-bound (paralelismo real)] tiempo: 0.116 s\n",
            "len(res_threads)= 4 len(res_processes)= 4\n"
          ]
        }
      ],
      "source": [
        "from concurrent.futures import ProcessPoolExecutor\n",
        "\n",
        "def cpu_intensivo(n: int) -> int:\n",
        "    # Simula trabajo pesado de CPU (sin esperas)\n",
        "    s = 0\n",
        "    for i in range(n):\n",
        "        s += (i * i) % 97\n",
        "    return s\n",
        "\n",
        "# Comparamos threads vs processes para CPU-bound\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "datos = [2_000_00] * 4  # cuatro trabajos similares\n",
        "\n",
        "with time_block(\"threads CPU-bound (GIL limita)\"):\n",
        "    with ThreadPoolExecutor() as ex:\n",
        "        res_threads = list(ex.map(cpu_intensivo, datos))\n",
        "\n",
        "with time_block(\"processes CPU-bound (paralelismo real)\"):\n",
        "    with ProcessPoolExecutor() as ex:\n",
        "        res_processes = list(ex.map(cpu_intensivo, datos))\n",
        "\n",
        "print(\"len(res_threads)=\", len(res_threads), \"len(res_processes)=\", len(res_processes))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1) Coste de serialización y chunksize (map con procesos)\n",
        "Los datos pasan entre procesos: suelen serializarse (copiarse) y eso consume tiempo. `map(..., chunksize=k)` agrupa elementos para reducir el número de mensajes.\n",
        "\n",
        "Qué observar:\n",
        "- Sin `chunksize`: muchos mensajes pequeños → más overhead.\n",
        "- Con `chunksize=20`: menos mensajes, mejor rendimiento (según tarea/datos).\n",
        "Sugerencia: mide con tus propios tamaños para encontrar el punto medio adecuado.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[processes map sin chunksize] tiempo: 0.611 s\n",
            "[processes map con chunksize=20] tiempo: 0.702 s\n",
            "200 200\n"
          ]
        }
      ],
      "source": [
        "from concurrent.futures import ProcessPoolExecutor\n",
        "\n",
        "# Función sencilla \"CPU-bound\" para ejemplificar\n",
        "\n",
        "def trabajo(x: int) -> int:\n",
        "    total = 0\n",
        "    for i in range(50_000):\n",
        "        total += (x + i) % 97\n",
        "    return total\n",
        "\n",
        "valores = list(range(200))\n",
        "\n",
        "with time_block(\"processes map sin chunksize\"):\n",
        "    with ProcessPoolExecutor() as ex:\n",
        "        res1 = list(ex.map(trabajo, valores))\n",
        "\n",
        "with time_block(\"processes map con chunksize=20\"):\n",
        "    with ProcessPoolExecutor() as ex:\n",
        "        res2 = list(ex.map(trabajo, valores, chunksize=20))\n",
        "\n",
        "print(len(res1), len(res2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Casos edge y combinaciones\n",
        "Aquí probamos situaciones que suelen generar dudas y las relacionamos con la teoría:\n",
        "- Asíncrono sin concurrencia efectiva.\n",
        "- Concurrencia sin asíncrono (solo hilos).\n",
        "- Paralelo “sin estilo asíncrono” (procesos con funciones normales).\n",
        "- Por qué “paralelo sin concurrencia” no tiene sentido.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1) Asíncrono sin concurrencia efectiva\n",
        "Usamos `asyncio` de forma secuencial (esperamos una tarea antes de iniciar la siguiente). No hay progreso en paralelo temporal. Compáralo con la versión que usa `gather`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4]\n",
            "[edge: async secuencial (5 * 0.2s)] tiempo: 1.505 s\n"
          ]
        }
      ],
      "source": [
        "# Reutilizamos run_async_secuencial definido antes\n",
        "async def _driver_edge_seq():\n",
        "    with time_block(\"edge: async secuencial (5 * 0.2s)\"):\n",
        "        print(await run_async_secuencial())\n",
        "\n",
        "await _driver_edge_seq()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2) Concurrente sin asíncrono (hilos)\n",
        "Cada tarea es bloqueante, pero como las ejecutamos en hilos, varias progresan durante el mismo periodo. Útil cuando el tiempo se va en esperas de E/S.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['fin 0', 'fin 1', 'fin 2', 'fin 3', 'fin 4']\n",
            "[edge: hilos bloqueantes (5 * 0.2s, 5 workers)] tiempo: 0.203 s\n"
          ]
        }
      ],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "import time\n",
        "\n",
        "def espera(i: int) -> str:\n",
        "    time.sleep(0.2)\n",
        "    return f\"fin {i}\"\n",
        "\n",
        "with time_block(\"edge: hilos bloqueantes (5 * 0.2s, 5 workers)\"):\n",
        "    with ThreadPoolExecutor(max_workers=5) as ex:\n",
        "        print(list(ex.map(espera, range(5))))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3) Paralelo sin “estilo asíncrono” (procesos)\n",
        "Cada tarea es secuencial (normal), pero se ejecutan al mismo tiempo en distintos núcleos gracias a procesos. Esto sí es paralelismo real de CPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[14998935, 14998965, 14998995, 14999025]\n",
            "[edge: procesos secuenciales en paralelo] tiempo: 0.131 s\n"
          ]
        }
      ],
      "source": [
        "from concurrent.futures import ProcessPoolExecutor\n",
        "\n",
        "def tarea_normal(x: int) -> int:\n",
        "    total = 0\n",
        "    for i in range(300_000):\n",
        "        total += (x + i) % 101\n",
        "    return total\n",
        "\n",
        "with time_block(\"edge: procesos secuenciales en paralelo\"):\n",
        "    with ProcessPoolExecutor() as ex:\n",
        "        print(list(ex.map(tarea_normal, range(4))))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Notas rápidas sobre memoria y GIL\n",
        "- Memoria (muy simplificado): hilos comparten memoria del proceso; procesos usan memoria separada. Compartir datos es fácil con hilos (pero hay que sincronizar); con procesos, hay que comunicar datos (más coste) o usar memoria compartida específica.\n",
        "- GIL (CPython): un cerrojo del intérprete por proceso. Limita el paralelismo de CPU con hilos, pero no afecta la concurrencia de E/S (al esperar, otros hilos avanzan). Para CPU-bound, prefiere procesos; para mucha E/S, hilos o asyncio funcionan bien.\n",
        "Sugerencia: mide siempre y elige el enfoque según el cuello de botella (CPU vs E/S).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Antes: ru_maxrss: 134472 KB (interprete del SO) | VmSize:\t 1058320 kB | VmRSS:\t   68712 kB\n",
            "Después: ru_maxrss: 134472 KB (interprete del SO) | VmSize:\t 1070680 kB | VmRSS:\t   81000 kB\n"
          ]
        }
      ],
      "source": [
        "# Observamos memoria (muy básico)\n",
        "\n",
        "def crear_lista_grande(n: int):\n",
        "    # Evitar tamaños enormes; esto es ilustrativo\n",
        "    return [i for i in range(n)]\n",
        "\n",
        "rss_kb_before, info_before = get_memory_info()\n",
        "print(\"Antes:\", info_before)\n",
        "\n",
        "data = crear_lista_grande(300_000)  # crea cierta presión de memoria\n",
        "rss_kb_after, info_after = get_memory_info()\n",
        "print(\"Después:\", info_after)\n",
        "\n",
        "# Evitar retener memoria innecesaria\n",
        "del data\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
